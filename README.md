# PM-RL: Reinforcement Learning for Portfolio Management

**The goal of this project is to consolidate and standardize *my own* existing work into an authoritative foundation by which future work can be confidently built.**

As always when it comes to software, these implementations draw heavily from primary sources and third-party implementations, and I make a good-faith effort to indicate as such in the code. No promises are made that these implementations are correct, or even theoretically sound for PM. I like to tinker, I like to make mistakes, and I love to learn from those mistakes.

My implementations have thus-far been disparate and notably lack unit-testing. My goals with this project extend beyond the academic domain; I also want to develop better programming habits and techniques. Thus, a significant focus here is on modularity, rigorous unit-testing, and improved documentation.

---

*This is by no means a replacement for existing RL implementations and frameworks. Any attempt to standardize existing standards just creates a new standard; the RL community does not need another. This is specially true in the financial subfield, where we have [FinRL](https://github.com/AI4Finance-Foundation/FinRL). In fact, such frameworks may be utilized here for validation and sanity checks.*

*See also: [CleanRL](https://github.com/vwxyzjn/cleanrl), [TorchRL](https://github.com/pytorch/rl), [ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL), [Google Dopamine](https://github.com/google/dopamine), [Meta Pearl](https://github.com/facebookresearch/Pearl), [OpenAI Baselines](https://github.com/openai/baselines), [Stable Baselines](https://github.com/DLR-RM/stable-baselines3), etc.*

---


